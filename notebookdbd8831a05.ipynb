{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10475602,"sourceType":"datasetVersion","datasetId":6486502},{"sourceId":10475672,"sourceType":"datasetVersion","datasetId":6486561},{"sourceId":249033,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":212861,"modelId":234500},{"sourceId":249325,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":213114,"modelId":234759},{"sourceId":254940,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":217975,"modelId":239697},{"sourceId":254945,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":217980,"modelId":239703}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rudravohra/notebookdbd8831a05?scriptVersionId=227954131\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:31.648713Z","iopub.execute_input":"2025-03-16T12:14:31.649101Z","iopub.status.idle":"2025-03-16T12:14:32.001625Z","shell.execute_reply.started":"2025-03-16T12:14:31.649068Z","shell.execute_reply":"2025-03-16T12:14:32.000931Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/rtdetr-35epoch/keras/default/1/epoch35.pt\n/kaggle/input/videos/videos/scenevideo_3.mp4\n/kaggle/input/videos/videos/scenevideo_11.mp4\n/kaggle/input/videos/videos/scenevideo_37.mp4\n/kaggle/input/videos/videos/scenevideo_21.mp4\n/kaggle/input/videos/videos/scenevideo_18.mp4\n/kaggle/input/videos/videos/scenevideo_7.mp4\n/kaggle/input/videos/videos/scenevideo_45.mp4\n/kaggle/input/videos/videos/scenevideo_5.mp4\n/kaggle/input/videos/videos/scenevideo_20.mp4\n/kaggle/input/videos/videos/scenevideo_26.mp4\n/kaggle/input/videos/videos/scenevideo_22.mp4\n/kaggle/input/videos/videos/scenevideo_1.mp4\n/kaggle/input/videos/videos/scenevideo_40.mp4\n/kaggle/input/videos/videos/scenevideo_23.mp4\n/kaggle/input/videos/videos/scenevideo_10.mp4\n/kaggle/input/videos/videos/scenevideo_50.mp4\n/kaggle/input/videos/videos/scenevideo_43.mp4\n/kaggle/input/videos/videos/scenevideo_35.mp4\n/kaggle/input/videos/videos/scenevideo_39.mp4\n/kaggle/input/videos/videos/scenevideo_17.mp4\n/kaggle/input/videos/videos/scenevideo_36.mp4\n/kaggle/input/videos/videos/scenevideo_31.mp4\n/kaggle/input/videos/videos/scenevideo_16.mp4\n/kaggle/input/videos/videos/scenevideo_13.mp4\n/kaggle/input/videos/videos/scenevideo_4.mp4\n/kaggle/input/videos/videos/scenevideo_12.mp4\n/kaggle/input/videos/videos/scenevideo_30.mp4\n/kaggle/input/videos/videos/scenevideo_34.mp4\n/kaggle/input/videos/videos/scenevideo_41.mp4\n/kaggle/input/videos/videos/scenevideo_15.mp4\n/kaggle/input/videos/videos/scenevideo_42.mp4\n/kaggle/input/videos/videos/scenevideo_44.mp4\n/kaggle/input/videos/videos/scenevideo_29.mp4\n/kaggle/input/videos/videos/scenevideo_48.mp4\n/kaggle/input/videos/videos/scenevideo_25.mp4\n/kaggle/input/videos/videos/scenevideo_46.mp4\n/kaggle/input/videos/videos/scenevideo_24.mp4\n/kaggle/input/videos/videos/scenevideo_19.mp4\n/kaggle/input/videos/videos/scenevideo_33.mp4\n/kaggle/input/videos/videos/scenevideo_9.mp4\n/kaggle/input/videos/videos/scenevideo_8.mp4\n/kaggle/input/videos/videos/scenevideo_38.mp4\n/kaggle/input/videos/videos/scenevideo_6.mp4\n/kaggle/input/videos/videos/scenevideo_32.mp4\n/kaggle/input/videos/videos/scenevideo_2.mp4\n/kaggle/input/videos/videos/scenevideo_47.mp4\n/kaggle/input/videos/videos/scenevideo_14.mp4\n/kaggle/input/videos/videos/scenevideo_49.mp4\n/kaggle/input/yolov8model/best (1).pt\n/kaggle/input/yolov8besr/keras/default/1/best (1).pt\n/kaggle/input/yolov9bestt/keras/default/1/yolov9best.pt\n/kaggle/input/rtdetr-best/keras/default/1/best (5).pt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install ultralytics opencv-python-headless","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:34.585801Z","iopub.execute_input":"2025-03-16T12:14:34.586278Z","iopub.status.idle":"2025-03-16T12:14:39.813752Z","shell.execute_reply.started":"2025-03-16T12:14:34.586211Z","shell.execute_reply":"2025-03-16T12:14:39.812912Z"}},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.3.91-py3-none-any.whl.metadata (35 kB)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\nRequirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\nRequirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (11.0.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\nRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nCollecting ultralytics-thop>=2.0.0 (from ultralytics)\n  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<=2.1.1,>=1.23.0->ultralytics) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.12.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<=2.1.1,>=1.23.0->ultralytics) (2024.2.0)\nDownloading ultralytics-8.3.91-py3-none-any.whl (949 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m949.2/949.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\nInstalling collected packages: ultralytics-thop, ultralytics\nSuccessfully installed ultralytics-8.3.91 ultralytics-thop-2.0.14\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import cv2\nimport os\nfrom ultralytics import YOLO,RTDETR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:39.814837Z","iopub.execute_input":"2025-03-16T12:14:39.81505Z","iopub.status.idle":"2025-03-16T12:14:43.971347Z","shell.execute_reply.started":"2025-03-16T12:14:39.81503Z","shell.execute_reply":"2025-03-16T12:14:43.970607Z"}},"outputs":[{"name":"stdout","text":"Creating new Ultralytics Settings v0.0.6 file ✅ \nView Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model = RTDETR(\"/kaggle/input/rtdetr-best/keras/default/1/best (5).pt\").to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T16:15:20.278942Z","iopub.execute_input":"2025-03-15T16:15:20.279579Z","iopub.status.idle":"2025-03-15T16:15:22.77527Z","shell.execute_reply.started":"2025-03-15T16:15:20.279538Z","shell.execute_reply":"2025-03-15T16:15:22.774566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"videos_path = '/kaggle/input/videos/videos/'\noutput_dir = \"/kaggle/working/testing\"  # Output directory for processed videos\nos.makedirs(output_dir, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T11:14:22.919547Z","iopub.execute_input":"2025-02-15T11:14:22.919851Z","iopub.status.idle":"2025-02-15T11:14:22.9236Z","shell.execute_reply.started":"2025-02-15T11:14:22.919827Z","shell.execute_reply":"2025-02-15T11:14:22.922819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_video(video_path, output_path):\n    cap = cv2.VideoCapture(video_path)\n\n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Set up video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Run YOLOv8 on the current frame\n        results = model(frame, conf=0.15, iou=0.7, max_det=500, agnostic_nms=True,verbose=False)\n        annotated_frame = results[0].plot()  # Annotate the frame with detections\n\n        # Write the annotated frame to the output video\n        out.write(annotated_frame)\n\n    cap.release()\n    out.release()\n    print(f\"Processed and saved: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T11:14:25.560155Z","iopub.execute_input":"2025-02-15T11:14:25.560478Z","iopub.status.idle":"2025-02-15T11:14:25.566156Z","shell.execute_reply.started":"2025-02-15T11:14:25.560448Z","shell.execute_reply":"2025-02-15T11:14:25.565064Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_files = [f for f in os.listdir(videos_path) if f.endswith('.mp4')]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T13:42:19.455008Z","iopub.execute_input":"2025-02-03T13:42:19.455304Z","iopub.status.idle":"2025-02-03T13:42:19.469427Z","shell.execute_reply.started":"2025-02-03T13:42:19.455281Z","shell.execute_reply":"2025-02-03T13:42:19.468762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(video_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:20:09.869145Z","iopub.execute_input":"2025-01-22T15:20:09.869444Z","iopub.status.idle":"2025-01-22T15:20:09.875746Z","shell.execute_reply.started":"2025-01-22T15:20:09.869419Z","shell.execute_reply":"2025-01-22T15:20:09.874647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T13:42:42.08025Z","iopub.execute_input":"2025-02-03T13:42:42.080588Z","iopub.status.idle":"2025-02-03T13:42:42.086564Z","shell.execute_reply.started":"2025-02-03T13:42:42.080564Z","shell.execute_reply":"2025-02-03T13:42:42.08588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for video in video_files:\n    input_path = os.path.join(videos_path, video)\n    output_path = os.path.join(output_dir, video.replace('.mp4', '_output.mp4'))\n    process_video(input_path, output_path)\nprint(\"Processing complete. All videos saved to:\", output_dir)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T15:20:09.992227Z","iopub.execute_input":"2025-01-22T15:20:09.992661Z","execution_failed":"2025-01-22T15:21:37.49Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_video(\"/kaggle/input/videos/videos/scenevideo_36.mp4\",\"/kaggle/working/testing/rtdetr.mp4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T11:33:45.784792Z","iopub.execute_input":"2025-02-15T11:33:45.785088Z","iopub.status.idle":"2025-02-15T11:37:12.29793Z","shell.execute_reply.started":"2025-02-15T11:33:45.785065Z","shell.execute_reply":"2025-02-15T11:37:12.297125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Path to the folder containing the processed videos\nfolder_path = '/kaggle/working/processed-videos'\nzip_file_path = '/kaggle/working/processed-videos.zip'\n\n# Create a zip file of the folder\nshutil.make_archive(zip_file_path.replace('.zip', ''), 'zip', folder_path)\n\nprint(f\"Zipped folder created: {zip_file_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T11:31:08.138954Z","iopub.execute_input":"2025-01-15T11:31:08.139269Z","iopub.status.idle":"2025-01-15T11:33:49.549808Z","shell.execute_reply.started":"2025-01-15T11:31:08.139245Z","shell.execute_reply":"2025-01-15T11:33:49.548978Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_iou(box1, box2):\n    x1 = max(box1[0], box2[0])\n    y1 = max(box1[1], box2[1])\n    x2 = min(box1[2], box2[2])\n    y2 = min(box1[3], box2[3])\n\n    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n\n    return inter_area / (box1_area + box2_area - inter_area)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:21:56.145202Z","iopub.execute_input":"2025-02-15T08:21:56.1455Z","iopub.status.idle":"2025-02-15T08:21:56.150971Z","shell.execute_reply.started":"2025-02-15T08:21:56.145476Z","shell.execute_reply":"2025-02-15T08:21:56.149952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def soft_nms(boxes, scores, sigma=0.8, Nt=0.4, threshold=0.01):\n    \n    N = len(scores)\n    for i in range(N):\n        for j in range(i + 1, N):\n            iou = compute_iou(boxes[i], boxes[j])\n            if iou > Nt:\n                scores[j] *= np.exp(-(iou ** 2) / sigma)  # Gaussian decay\n\n    keep = np.where(scores > threshold)[0]\n    return keep","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:21:58.307777Z","iopub.execute_input":"2025-02-15T08:21:58.308074Z","iopub.status.idle":"2025-02-15T08:21:58.312805Z","shell.execute_reply.started":"2025-02-15T08:21:58.308052Z","shell.execute_reply":"2025-02-15T08:21:58.311992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model=YOLO(\"/kaggle/input/yolov8besr/keras/default/1/best (1).pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:22:42.5492Z","iopub.execute_input":"2025-02-15T08:22:42.549553Z","iopub.status.idle":"2025-02-15T08:22:42.600268Z","shell.execute_reply.started":"2025-02-15T08:22:42.549521Z","shell.execute_reply":"2025-02-15T08:22:42.599603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_video_nms(video_path, output_path):\n\n    cap = cv2.VideoCapture(video_path)\n\n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Set up video writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    # Generate unique colors for each class\n    num_classes = len(model.names)\n    class_colors = {i: tuple(np.random.randint(0, 255, 3).tolist()) for i in range(num_classes)}\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Run YOLOv8 on the current frame\n        results = model(frame,conf=0.18, max_det=500, agnostic_nms=True,verbose=False)\n\n        # Collect boxes, scores, and labels\n        boxes, scores, labels = [], [], []\n        for r in results:\n            for box, score, cls in zip(r.boxes.xyxy.cpu().numpy(), r.boxes.conf.cpu().numpy(), r.boxes.cls.cpu().numpy()):\n                boxes.append(box)\n                scores.append(score)\n                labels.append(cls)\n\n        if len(boxes) > 0:\n            # Convert boxes and scores to numpy arrays\n            boxes = np.array(boxes)\n            scores = np.array(scores)\n\n            # Draw each detection on the frame\n            for box, score, label in zip(boxes, scores, labels):\n                x1, y1, x2, y2 = map(int, box)\n                class_id = int(label)\n\n                # Get class name and color\n                class_name = model.names[class_id]\n                color = class_colors[class_id]\n\n                # Draw bounding box\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n                # Prepare label text and size\n                label_text = f\"{class_name}: {score:.2f}\"\n                (text_width, text_height), baseline = cv2.getTextSize(\n                    label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2\n                )\n\n                # Draw a filled rectangle as the background for the label\n                cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n\n                # Put the label text on top of the background\n                cv2.putText(\n                    frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2\n                )\n\n        # Write the annotated frame to the output video\n        out.write(frame)\n\n    # Release resources\n    cap.release()\n    out.release()\n    print(f\"Processed and saved: {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:22:46.194866Z","iopub.execute_input":"2025-02-15T08:22:46.195172Z","iopub.status.idle":"2025-02-15T08:22:46.204235Z","shell.execute_reply.started":"2025-02-15T08:22:46.195148Z","shell.execute_reply":"2025-02-15T08:22:46.203224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = \"/kaggle/input/videos/videos/scenevideo_36.mp4\"  # Path to input video\noutput_path = \"/kaggle/working/testing/soft-nms.mp4\"  # Path to save the output video","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:22:51.732977Z","iopub.execute_input":"2025-02-15T08:22:51.7333Z","iopub.status.idle":"2025-02-15T08:22:51.736938Z","shell.execute_reply.started":"2025-02-15T08:22:51.733274Z","shell.execute_reply":"2025-02-15T08:22:51.736009Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_video_nms(video_path, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:22:55.348332Z","iopub.execute_input":"2025-02-15T08:22:55.348664Z","iopub.status.idle":"2025-02-15T08:23:26.501545Z","shell.execute_reply.started":"2025-02-15T08:22:55.348635Z","shell.execute_reply":"2025-02-15T08:23:26.500765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Path to the ZIP file\nfile_to_remove = \"/kaggle/working/yolov8_bifpn.yaml\"\n\n# Check if it's a file and remove it\nif os.path.isfile(file_to_remove):\n    os.remove(file_to_remove)\n    print(f\"File '{file_to_remove}' has been removed successfully.\")\nelse:\n    print(f\"File '{file_to_remove}' does not exist or is not a file.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T13:41:47.070131Z","iopub.execute_input":"2025-02-03T13:41:47.07045Z","iopub.status.idle":"2025-02-03T13:41:47.075511Z","shell.execute_reply.started":"2025-02-03T13:41:47.070427Z","shell.execute_reply":"2025-02-03T13:41:47.074602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# Directory to remove\ndir_to_remove = \"/kaggle/working/testing\"\n\n# Check if the directory exists before removing\nif os.path.exists(dir_to_remove):\n    shutil.rmtree(dir_to_remove)\n    print(f\"Directory '{dir_to_remove}' has been removed successfully.\")\nelse:\n    print(f\"Directory '{dir_to_remove}' does not exist.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T13:58:37.985563Z","iopub.execute_input":"2025-02-03T13:58:37.985884Z","iopub.status.idle":"2025-02-03T13:58:37.991584Z","shell.execute_reply.started":"2025-02-03T13:58:37.98586Z","shell.execute_reply":"2025-02-03T13:58:37.990842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install ensemble_boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:45.328278Z","iopub.execute_input":"2025-03-16T12:14:45.328742Z","iopub.status.idle":"2025-03-16T12:14:48.785631Z","shell.execute_reply.started":"2025-03-16T12:14:45.328717Z","shell.execute_reply":"2025-03-16T12:14:48.784528Z"}},"outputs":[{"name":"stdout","text":"Collecting ensemble_boxes\n  Downloading ensemble_boxes-1.0.9-py3-none-any.whl.metadata (728 bytes)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ensemble_boxes) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ensemble_boxes) (2.2.2)\nRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from ensemble_boxes) (0.60.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->ensemble_boxes) (0.43.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->ensemble_boxes) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->ensemble_boxes) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->ensemble_boxes) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->ensemble_boxes) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->ensemble_boxes) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->ensemble_boxes) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ensemble_boxes) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ensemble_boxes) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ensemble_boxes) (2024.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ensemble_boxes) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ensemble_boxes) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ensemble_boxes) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->ensemble_boxes) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->ensemble_boxes) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->ensemble_boxes) (2024.2.0)\nDownloading ensemble_boxes-1.0.9-py3-none-any.whl (23 kB)\nInstalling collected packages: ensemble_boxes\nSuccessfully installed ensemble_boxes-1.0.9\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from ensemble_boxes import weighted_boxes_fusion\nfrom ultralytics import RTDETR","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:48.787163Z","iopub.execute_input":"2025-03-16T12:14:48.787559Z","iopub.status.idle":"2025-03-16T12:14:49.844749Z","shell.execute_reply.started":"2025-03-16T12:14:48.787531Z","shell.execute_reply":"2025-03-16T12:14:49.844095Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load YOLOv8 and YOLOv9 models\nmodel_v8 = YOLO(\"/kaggle/input/yolov8besr/keras/default/1/best (1).pt\")  # Replace with your YOLOv8 model path\nmodel_v9 = YOLO(\"/kaggle/input/yolov9bestt/keras/default/1/yolov9best.pt\")  # Replace with your YOLOv9 model path\nmodel_rtdetr = RTDETR(\"/kaggle/input/rtdetr-best/keras/default/1/best (5).pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:49.845959Z","iopub.execute_input":"2025-03-16T12:14:49.846176Z","iopub.status.idle":"2025-03-16T12:14:52.308172Z","shell.execute_reply.started":"2025-03-16T12:14:49.846153Z","shell.execute_reply":"2025-03-16T12:14:52.307249Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class_names=['trak', 'cyclist', 'bike', 'tempo', 'car', 'zeep', 'toto', 'e-rickshaw', 'auto-rickshaw', 'bus', 'van', 'cycle-rickshaw', 'person', 'taxi']\nclass_colors = {\n    'trak': (255, 0, 0),            # Red\n    'cyclist': (0, 255, 0),         # Green\n    'bike': (0, 0, 255),            # Blue\n    'tempo': (255, 255, 0),         # Cyan\n    'car': (255, 0, 255),           # Magenta\n    'zeep': (0, 255, 255),          # Yellow\n    'toto': (128, 0, 128),          # Purple\n    'e-rickshaw': (255, 165, 0),    # Orange\n    'auto-rickshaw': (0, 128, 128), # Teal\n    'bus': (128, 128, 0),           # Olive\n    'van': (75, 0, 130),            # Indigo\n    'cycle-rickshaw': (255, 192, 203), # Pink\n    'person': (0, 0, 0),            # Black\n    'taxi': (192, 192, 192)         # Silver\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:52.309274Z","iopub.execute_input":"2025-03-16T12:14:52.309597Z","iopub.status.idle":"2025-03-16T12:14:52.314511Z","shell.execute_reply.started":"2025-03-16T12:14:52.309575Z","shell.execute_reply":"2025-03-16T12:14:52.313713Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def filter_large_boxes(boxes, scores, labels, max_area=0.9):\n    filtered_boxes = []\n    filtered_scores = []\n    filtered_labels = []\n\n    for i, box in enumerate(boxes):\n        x1, y1, x2, y2 = box\n        area = (x2 - x1) * (y2 - y1) / (width * height)\n\n        if area <= max_area:\n            filtered_boxes.append(box)\n            filtered_scores.append(scores[i])\n            filtered_labels.append(labels[i])\n\n    return np.array(filtered_boxes), np.array(filtered_scores), np.array(filtered_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:52.315165Z","iopub.execute_input":"2025-03-16T12:14:52.315471Z","iopub.status.idle":"2025-03-16T12:14:52.330063Z","shell.execute_reply.started":"2025-03-16T12:14:52.315451Z","shell.execute_reply":"2025-03-16T12:14:52.329222Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Adaptive-NMS**","metadata":{}},{"cell_type":"code","source":"import torch,torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:52.956629Z","iopub.execute_input":"2025-03-16T12:14:52.956913Z","iopub.status.idle":"2025-03-16T12:14:54.81719Z","shell.execute_reply.started":"2025-03-16T12:14:52.956887Z","shell.execute_reply":"2025-03-16T12:14:54.816323Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def adaptive_nms(boxes, scores, labels, base_iou=0.01, min_iou=0.05, max_iou=0.95):\n    if len(boxes) == 0:\n        return np.array([]), np.array([]), np.array([])\n\n    # Convert to torch tensors\n    boxes = torch.tensor(boxes, dtype=torch.float32)\n    scores = torch.tensor(scores, dtype=torch.float32)\n    labels = torch.tensor(labels, dtype=torch.int64)\n\n    # Scale IoU threshold dynamically and convert to tensor\n    iou_thresh = torch.tensor(min_iou + (max_iou - min_iou) * scores.numpy(), dtype=torch.float32, device=boxes.device)\n\n    keep_indices = []\n\n    for cls in torch.unique(labels):  # Apply per-class NMS\n        class_mask = labels == cls\n        class_boxes = boxes[class_mask]\n        class_scores = scores[class_mask]\n        class_iou_thresh = iou_thresh[class_mask]\n\n        if len(class_boxes) == 1:  # ✅ Handle case with only one box per class\n            keep_indices.append(torch.where(class_mask)[0].item())\n            continue\n\n        # Sort by confidence\n        sorted_indices = torch.argsort(class_scores, descending=True)\n        class_boxes = class_boxes[sorted_indices]\n        class_scores = class_scores[sorted_indices]\n        class_iou_thresh = class_iou_thresh[sorted_indices]\n\n        while len(class_boxes) > 0:\n            keep_indices.append(sorted_indices[0].item())  # Keep highest confidence box\n            if len(class_boxes) == 1:\n                break\n            \n            # Compute IoU with remaining boxes\n            ious = torchvision.ops.box_iou(class_boxes[:1], class_boxes[1:])[0]\n            \n            # Ensure correct indexing (convert to torch tensor)\n            remaining_mask = ious < class_iou_thresh[1:].to(ious.device)\n\n            # ✅ Avoid indexing errors by checking length before slicing\n            if len(class_boxes) > 1:\n                class_boxes = class_boxes[1:][remaining_mask]\n                class_scores = class_scores[1:][remaining_mask]\n                class_iou_thresh = class_iou_thresh[1:][remaining_mask]\n            else:\n                break\n\n    # Apply the kept indices\n    return boxes[keep_indices].numpy(), scores[keep_indices].numpy(), labels[keep_indices].numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T05:30:49.221797Z","iopub.execute_input":"2025-03-13T05:30:49.222096Z","iopub.status.idle":"2025-03-13T05:30:49.229859Z","shell.execute_reply.started":"2025-03-13T05:30:49.222074Z","shell.execute_reply":"2025-03-13T05:30:49.228997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_frame_adaptive_nms(frame):\n    height, width = frame.shape[:2]\n\n    # Convert frame to RGB as required by the models\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # Get predictions from YOLOv8\n    results_v8 = model_v8.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_v8 = results_v8.boxes.xyxy.cpu().numpy()\n    scores_v8 = results_v8.boxes.conf.cpu().numpy()\n    labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n    # Get predictions from YOLOv9\n    results_v9 = model_v9.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_v9 = results_v9.boxes.xyxy.cpu().numpy()\n    scores_v9 = results_v9.boxes.conf.cpu().numpy()\n    labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n\n    # Get predictions from RT-DETR\n    results_rtdetr = model_rtdetr.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_rtdetr = results_rtdetr.boxes.xyxy.cpu().numpy()\n    scores_rtdetr = results_rtdetr.boxes.conf.cpu().numpy()\n    labels_rtdetr = results_rtdetr.boxes.cls.cpu().numpy().astype(int)\n\n    # ✅ Apply filtering on RT-DETR boxes **before** normalizing\n    boxes_rtdetr, scores_rtdetr, labels_rtdetr = filter_large_boxes(boxes_rtdetr, scores_rtdetr, labels_rtdetr)\n\n    # ✅ Normalize after filtering\n    boxes_v8 = np.clip(boxes_v8 / [width, height, width, height], 0, 1)\n    boxes_v9 = np.clip(boxes_v9 / [width, height, width, height], 0, 1)\n\n    # Only normalize RT-DETR boxes if they exist\n    if boxes_rtdetr.shape[0] > 0:\n        boxes_rtdetr = np.clip(boxes_rtdetr / [width, height, width, height], 0, 1)\n\n    # ✅ Dynamically handle WBF weights\n    all_boxes, all_scores, all_labels = [boxes_v8.tolist(), boxes_v9.tolist()], [scores_v8.tolist(), scores_v9.tolist()], [labels_v8.tolist(), labels_v9.tolist()]\n    weights = [14, 8]  # Weights for YOLOv8 and YOLOv9\n\n    if boxes_rtdetr.shape[0] > 0:\n        all_boxes.append(boxes_rtdetr.tolist())\n        all_scores.append(scores_rtdetr.tolist())\n        all_labels.append(labels_rtdetr.tolist())\n        weights.append(0.5)  # Add RT-DETR weight only if predictions exist\n\n    # Apply Weighted Boxes Fusion\n    fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels,\n        iou_thr=0.6, skip_box_thr=0.30, weights=weights\n    )\n\n    # Apply Adaptive NMS after WBF\n    fused_boxes, fused_scores, fused_labels = adaptive_nms(\n        fused_boxes, fused_scores, fused_labels    \n    )\n\n    # ✅ Handle case where no predictions remain\n    if len(fused_boxes) == 0:\n        return frame\n\n    conf_thresh = 0.1  # Set confidence threshold again after WBF + NMS\n    valid_indices = np.where(fused_scores >= conf_thresh)[0]\n\n    fused_boxes = np.array(fused_boxes)[valid_indices]\n    fused_scores = np.array(fused_scores)[valid_indices]\n    fused_labels = np.array(fused_labels)[valid_indices]\n\n    # **Denormalize** boxes back to pixel coordinates\n    fused_boxes = (fused_boxes * [width, height, width, height]).astype(int)\n\n    # Draw bounding boxes on frame\n    for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n        x1, y1, x2, y2 = box\n        class_name = class_names[int(label)] if int(label) < len(class_names) else f'Class {int(label)}'\n        color = class_colors.get(class_name, (0, 255, 0))  # Default color if missing\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)\n        label_text = f'{class_name}: {score:.2f}'\n        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n        cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n\n    return frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T05:30:53.306807Z","iopub.execute_input":"2025-03-13T05:30:53.307106Z","iopub.status.idle":"2025-03-13T05:30:53.318872Z","shell.execute_reply.started":"2025-03-13T05:30:53.307081Z","shell.execute_reply":"2025-03-13T05:30:53.318053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_36.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing1/test36-adaptive_nms-1.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T05:30:55.382099Z","iopub.execute_input":"2025-03-13T05:30:55.382443Z","iopub.status.idle":"2025-03-13T05:30:55.386302Z","shell.execute_reply.started":"2025-03-13T05:30:55.382423Z","shell.execute_reply":"2025-03-13T05:30:55.385347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cap = cv2.VideoCapture(input_video_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Process frame with WBF\n    processed_frame = process_frame_adaptive_nms(frame)\n\n    # Write frame to output video\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f'Output video saved to {output_video_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T05:31:10.549289Z","iopub.execute_input":"2025-03-13T05:31:10.549633Z","iopub.status.idle":"2025-03-13T05:35:01.128371Z","shell.execute_reply.started":"2025-03-13T05:31:10.549602Z","shell.execute_reply":"2025-03-13T05:35:01.127291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SOFT-NMS","metadata":{}},{"cell_type":"code","source":"def compute_iou(box, boxes):\n    \"\"\"Computes IoU between one box and an array of boxes.\"\"\"\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n\n    inter_area = np.maximum(0, x2 - x1) * np.maximum(0, y2 - y1)\n    box_area = (box[2] - box[0]) * (box[3] - box[1])\n    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n    union_area = box_area + boxes_area - inter_area\n\n    return inter_area / (union_area + 1e-6)  # Avoid division by zero","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:58.168401Z","iopub.execute_input":"2025-03-16T12:14:58.168722Z","iopub.status.idle":"2025-03-16T12:14:58.174378Z","shell.execute_reply.started":"2025-03-16T12:14:58.168696Z","shell.execute_reply":"2025-03-16T12:14:58.173421Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def soft_nms(boxes, scores, labels, sigma=0.62, iou_threshold=0.40, score_threshold=0.12):\n    \n    if len(boxes) == 0:\n        return boxes, scores, labels\n\n    # Convert inputs to NumPy arrays\n    boxes = np.array(boxes, dtype=np.float32)\n    scores = np.array(scores, dtype=np.float32)\n    labels = np.array(labels, dtype=np.int32)\n\n    keep_boxes = []\n    keep_scores = []\n    keep_labels = []\n\n    while len(scores) > 0:\n        # Select the highest confidence box\n        max_idx = np.argmax(scores)\n        max_box = boxes[max_idx]\n        max_score = scores[max_idx]\n        max_label = labels[max_idx]\n\n        keep_boxes.append(max_box)\n        keep_scores.append(max_score)\n        keep_labels.append(max_label)\n\n        # Compute IoU with the remaining boxes\n        ious = compute_iou(max_box, boxes)\n\n        # Apply Gaussian penalty to scores based on IoU\n        scores *= np.exp(-(ious ** 2) / sigma)\n\n        # Remove low-confidence boxes\n        keep_mask = scores >= score_threshold\n        boxes = boxes[keep_mask]\n        scores = scores[keep_mask]\n        labels = labels[keep_mask]\n\n    return np.array(keep_boxes), np.array(keep_scores), np.array(keep_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:14:58.519182Z","iopub.execute_input":"2025-03-16T12:14:58.519439Z","iopub.status.idle":"2025-03-16T12:14:58.525161Z","shell.execute_reply.started":"2025-03-16T12:14:58.519419Z","shell.execute_reply":"2025-03-16T12:14:58.524371Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_1.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing1/13march(1).mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:15:01.245449Z","iopub.execute_input":"2025-03-16T12:15:01.24578Z","iopub.status.idle":"2025-03-16T12:15:01.249865Z","shell.execute_reply.started":"2025-03-16T12:15:01.245757Z","shell.execute_reply":"2025-03-16T12:15:01.248876Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def process_frame(frame):\n    height, width = frame.shape[:2]\n\n    # Convert frame to RGB as required by the models\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # Get predictions from YOLOv8\n    results_v8 = model_v8.predict(img_rgb, verbose=False, agnostic_nms=False)[0]\n    boxes_v8 = results_v8.boxes.xyxy.cpu().numpy()\n    scores_v8 = results_v8.boxes.conf.cpu().numpy()\n    labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n    # Get predictions from YOLOv9\n    results_v9 = model_v9.predict(img_rgb, verbose=False, agnostic_nms=False)[0]\n    boxes_v9 = results_v9.boxes.xyxy.cpu().numpy()\n    scores_v9 = results_v9.boxes.conf.cpu().numpy()\n    labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n\n    # Get predictions from RT-DETR\n    results_rtdetr = model_rtdetr.predict(img_rgb, verbose=False, agnostic_nms=False,conf=0.8,iou=0.2)[0]\n    boxes_rtdetr = results_rtdetr.boxes.xyxy.cpu().numpy()\n    scores_rtdetr = results_rtdetr.boxes.conf.cpu().numpy()\n    labels_rtdetr = results_rtdetr.boxes.cls.cpu().numpy().astype(int)\n\n    # ✅ Apply filtering on RT-DETR boxes **before** normalizing\n    boxes_rtdetr, scores_rtdetr, labels_rtdetr = filter_large_boxes(boxes_rtdetr, scores_rtdetr, labels_rtdetr)\n\n    # ✅ Normalize after filtering\n    boxes_v8 = np.clip(boxes_v8 / [width, height, width, height], 0, 1)\n    boxes_v9 = np.clip(boxes_v9 / [width, height, width, height], 0, 1)\n\n    if boxes_rtdetr.shape[0] > 0:\n        boxes_rtdetr = np.clip(boxes_rtdetr / [width, height, width, height], 0, 1)\n\n    # ✅ Dynamically handle WBF weights\n    all_boxes, all_scores, all_labels = [boxes_v8.tolist(), boxes_v9.tolist()], [scores_v8.tolist(), scores_v9.tolist()], [labels_v8.tolist(), labels_v9.tolist()]\n    weights = [40, 5]\n\n    if boxes_rtdetr.shape[0] > 0:\n        all_boxes.append(boxes_rtdetr.tolist())\n        all_scores.append(scores_rtdetr.tolist())\n        all_labels.append(labels_rtdetr.tolist())\n        weights.append(0.00000001)\n\n    # Apply Weighted Boxes Fusion\n    fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels,\n        iou_thr=0.63, skip_box_thr=0.26, weights=weights\n    )\n\n    conf_thresh = 0.17\n    valid_indices = np.where(fused_scores >= conf_thresh)[0]\n\n    fused_boxes = np.array(fused_boxes)[valid_indices]\n    fused_scores = np.array(fused_scores)[valid_indices]\n    fused_labels = np.array(fused_labels)[valid_indices]\n\n    # **Denormalize** boxes back to pixel coordinates\n    fused_boxes = (fused_boxes * [width, height, width, height]).astype(int)\n\n    # ✅ Apply Soft-NMS **Here**\n    fused_boxes, fused_scores, fused_labels = soft_nms(\n        fused_boxes, fused_scores, fused_labels,\n       score_threshold=conf_thresh\n    )\n# Draw bounding boxes on frame\n    for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n        x1, y1, x2, y2 = map(int, box)  # Ensure coordinates are integers\n        \n        class_name = class_names[int(label)] if int(label) < len(class_names) else f'Class {int(label)}'\n        \n        color = class_colors.get(class_name, (0, 255, 0))  # Default color if missing\n        if not isinstance(color, tuple) or len(color) != 3:\n            color = (0, 255, 0)  # Fallback to green\n        color = tuple(map(int, color))  # Ensure valid color format\n        \n        #print(f\"x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}, color: {color}\")  # Debugging\n        \n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 3)  # ✅ No error\n        label_text = f'{class_name}: {score:.2f}'\n        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n        cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)\n        \n    return frame    \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:15:02.25203Z","iopub.execute_input":"2025-03-16T12:15:02.252379Z","iopub.status.idle":"2025-03-16T12:15:02.265638Z","shell.execute_reply.started":"2025-03-16T12:15:02.252352Z","shell.execute_reply":"2025-03-16T12:15:02.264543Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"cap = cv2.VideoCapture(input_video_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Process frame with WBF\n    processed_frame = process_frame(frame)\n\n    # Write frame to output video\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f'Output video saved to {output_video_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T06:16:03.206689Z","iopub.execute_input":"2025-03-13T06:16:03.207001Z","iopub.status.idle":"2025-03-13T06:21:34.061867Z","shell.execute_reply.started":"2025-03-13T06:16:03.206974Z","shell.execute_reply":"2025-03-13T06:21:34.060982Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Tracking","metadata":{}},{"cell_type":"code","source":"!pip install deep_sort_realtime\n!pip install supervision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:43:53.337259Z","iopub.execute_input":"2025-02-16T06:43:53.337653Z","iopub.status.idle":"2025-02-16T06:44:00.0047Z","shell.execute_reply.started":"2025-02-16T06:43:53.337619Z","shell.execute_reply":"2025-02-16T06:44:00.003608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import supervision as sv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:00.005964Z","iopub.execute_input":"2025-02-16T06:44:00.006294Z","iopub.status.idle":"2025-02-16T06:44:00.1761Z","shell.execute_reply.started":"2025-02-16T06:44:00.00627Z","shell.execute_reply":"2025-02-16T06:44:00.175385Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ensemble_boxes import weighted_boxes_fusion","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:00.177675Z","iopub.execute_input":"2025-02-16T06:44:00.178326Z","iopub.status.idle":"2025-02-16T06:44:00.18211Z","shell.execute_reply.started":"2025-02-16T06:44:00.1783Z","shell.execute_reply":"2025-02-16T06:44:00.181053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"byte_tracker = sv.ByteTrack(\n    track_activation_threshold=0.01,  # ✅ Lower threshold to keep more objects\n    lost_track_buffer=30,\n    minimum_matching_threshold=0.1,  # ✅ Allow looser matches\n    frame_rate=30,\n    minimum_consecutive_frames=1  # ✅ No need to see object multiple times\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:00.183201Z","iopub.execute_input":"2025-02-16T06:44:00.183553Z","iopub.status.idle":"2025-02-16T06:44:00.195047Z","shell.execute_reply.started":"2025-02-16T06:44:00.183522Z","shell.execute_reply":"2025-02-16T06:44:00.194258Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"box_annotator = sv.BoxAnnotator(thickness=3)\nlabel_annotator = sv.LabelAnnotator(text_thickness=2, text_scale=1.5, text_color=sv.Color.BLACK)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:00.195831Z","iopub.execute_input":"2025-02-16T06:44:00.196059Z","iopub.status.idle":"2025-02-16T06:44:00.205753Z","shell.execute_reply.started":"2025-02-16T06:44:00.19604Z","shell.execute_reply":"2025-02-16T06:44:00.20496Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_frame_track(frame):\n    height, width = frame.shape[:2]\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # YOLOv8 predictions\n    results_v8 = model_v8.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_v8 = results_v8.boxes.xyxy.cpu().numpy()\n    scores_v8 = results_v8.boxes.conf.cpu().numpy()\n    labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n    # YOLOv9 predictions\n    results_v9 = model_v9.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_v9 = results_v9.boxes.xyxy.cpu().numpy()\n    scores_v9 = results_v9.boxes.conf.cpu().numpy()\n    labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n\n    # Normalize boxes\n    boxes_v8 = np.clip(boxes_v8 / [width, height, width, height], 0, 1)\n    boxes_v9 = np.clip(boxes_v9 / [width, height, width, height], 0, 1)\n\n    # Apply Weighted Boxes Fusion\n    all_boxes = [boxes_v8.tolist(), boxes_v9.tolist()]\n    all_scores = [scores_v8.tolist(), scores_v9.tolist()]\n    all_labels = [labels_v8.tolist(), labels_v9.tolist()]\n    weights = [10, 8]\n\n    fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels, iou_thr=0.63, skip_box_thr=0.26, weights=weights\n    )\n\n    conf_thresh = 0.17\n    valid_indices = np.where(fused_scores >= conf_thresh)[0]\n    fused_boxes = np.array(fused_boxes)[valid_indices]\n    fused_scores = np.array(fused_scores)[valid_indices]\n    fused_labels = np.array(fused_labels)[valid_indices]\n\n    # Denormalize boxes\n    fused_boxes = (fused_boxes * [width, height, width, height]).astype(int)\n\n    # Convert detections to Supervision format\n    detections = sv.Detections(\n        xyxy=fused_boxes,\n        confidence=fused_scores,\n        class_id=fused_labels.astype(int)  # ✅ Ensure class_id is integer\n    )\n\n    # 🔍 Debugging: Print before tracking\n    print(f\"Before Tracking: {len(detections)} objects detected by YOLO (after WBF)\")\n\n    # Apply ByteTrack\n    detections = byte_tracker.update_with_detections(detections)\n\n    # 🔍 Debugging: Print after tracking\n    print(f\"After Tracking: {len(detections)} objects remaining after ByteTrack\")\n\n    # Format labels\n    labels = [\n        f\"#{tracker_id} {model_v8.model.names[class_id]} {confidence:.2f}\"\n        for confidence, class_id, tracker_id\n        in zip(detections.confidence, detections.class_id, detections.tracker_id)\n    ]\n\n    # Draw detections\n    annotated_frame = frame.copy()\n    annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n    annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n\n    return annotated_frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:38:54.881043Z","iopub.execute_input":"2025-02-16T05:38:54.88132Z","iopub.status.idle":"2025-02-16T05:38:54.891476Z","shell.execute_reply.started":"2025-02-16T05:38:54.8813Z","shell.execute_reply":"2025-02-16T05:38:54.890656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_36.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing/test36-(1)-bytetracker.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:01:11.927046Z","iopub.execute_input":"2025-02-15T18:01:11.927376Z","iopub.status.idle":"2025-02-15T18:01:11.930945Z","shell.execute_reply.started":"2025-02-15T18:01:11.927348Z","shell.execute_reply":"2025-02-15T18:01:11.930136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" #Open video\ncap = cv2.VideoCapture(input_video_path)\nframe_width, frame_height = int(cap.get(3)), int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Set up video writer\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    processed_frame = process_frame_track(frame)\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\n\nprint(f\"✅ Processed video saved at: {output_video_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install deep-sort-realtime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T18:38:02.869707Z","iopub.execute_input":"2025-02-15T18:38:02.870044Z","iopub.status.idle":"2025-02-15T18:38:06.178078Z","shell.execute_reply.started":"2025-02-15T18:38:02.870018Z","shell.execute_reply":"2025-02-15T18:38:06.176951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deep_sort_realtime.deepsort_tracker import DeepSort\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:01.825277Z","iopub.execute_input":"2025-02-16T06:44:01.825622Z","iopub.status.idle":"2025-02-16T06:44:01.83255Z","shell.execute_reply.started":"2025-02-16T06:44:01.825593Z","shell.execute_reply":"2025-02-16T06:44:01.831802Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"deep_sort_tracker = DeepSort(\n    max_age=5,           # Remove lost tracks faster\n    embedder=\"mobilenet\",\n    n_init=4,             # Require more detections before confirming a track\n    max_iou_distance=0.72  # Keep stable, or slightly increase if needed\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:03.18267Z","iopub.execute_input":"2025-02-16T06:44:03.182951Z","iopub.status.idle":"2025-02-16T06:44:04.770537Z","shell.execute_reply.started":"2025-02-16T06:44:03.18293Z","shell.execute_reply":"2025-02-16T06:44:04.769785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_frame_track_deepsort(frame):\n    height, width = frame.shape[:2]\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # YOLOv8 predictions\n    results_v8 = model_v8.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_v8 = results_v8.boxes.xyxy.cpu().numpy()\n    scores_v8 = results_v8.boxes.conf.cpu().numpy()\n    labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n    # YOLOv9 predictions\n    results_v9 = model_v9.predict(img_rgb, verbose=False, agnostic_nms=True)[0]\n    boxes_v9 = results_v9.boxes.xyxy.cpu().numpy()\n    scores_v9 = results_v9.boxes.conf.cpu().numpy()\n    labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n\n    # Normalize boxes\n    boxes_v8 = np.clip(boxes_v8 / [width, height, width, height], 0, 1)\n    boxes_v9 = np.clip(boxes_v9 / [width, height, width, height], 0, 1)\n\n    # Apply Weighted Boxes Fusion\n    all_boxes = [boxes_v8.tolist(), boxes_v9.tolist()]\n    all_scores = [scores_v8.tolist(), scores_v9.tolist()]\n    all_labels = [labels_v8.tolist(), labels_v9.tolist()]\n    weights = [10, 8]\n\n    fused_boxes, fused_scores, fused_labels = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels, iou_thr=0.63, skip_box_thr=0.26, weights=weights\n    )\n\n    conf_thresh = 0.17\n    valid_indices = np.where(fused_scores >= conf_thresh)[0]\n    fused_boxes = np.array(fused_boxes)[valid_indices]\n    fused_scores = np.array(fused_scores)[valid_indices]\n    fused_labels = np.array(fused_labels)[valid_indices]\n\n    # Denormalize boxes\n    fused_boxes = (fused_boxes * [width, height, width, height]).astype(int)\n\n    # Convert detections to DeepSORT format\n    deepsort_detections = []\n    for box, score, label in zip(fused_boxes, fused_scores, fused_labels):\n        x1, y1, x2, y2 = box\n        deepsort_detections.append(([x1, y1, x2 - x1, y2 - y1], score, label))\n\n    # Apply DeepSORT tracking\n    tracked_objects = deep_sort_tracker.update_tracks(deepsort_detections, frame=frame)\n\n    # Prepare tracking results\n    detections = []\n    tracked_labels = []  # ✅ Store class_id for Supervision\n    labels = []\n    for track in tracked_objects:\n        if not track.is_confirmed():\n            continue\n        x1, y1, x2, y2 = track.to_ltrb()\n        detections.append([x1, y1, x2, y2])\n        tracked_labels.append(track.det_class)  # ✅ Store class_id\n        labels.append(f\"ID {track.track_id} {model_v8.model.names[track.det_class]}\")\n\n    annotated_frame = frame.copy()\n\n    if len(detections) > 0:\n        detections = sv.Detections(\n            xyxy=np.array(detections),\n            class_id=np.array(tracked_labels, dtype=int)  # ✅ Include class_id\n        )\n    \n        annotated_frame = box_annotator.annotate(scene=annotated_frame, detections=detections)\n        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)\n\n\n    return annotated_frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T06:44:06.221258Z","iopub.execute_input":"2025-02-16T06:44:06.221587Z","iopub.status.idle":"2025-02-16T06:44:06.23185Z","shell.execute_reply.started":"2025-02-16T06:44:06.221562Z","shell.execute_reply":"2025-02-16T06:44:06.231068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_36.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing/test36-(6)-deepsort.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T07:01:58.254613Z","iopub.execute_input":"2025-02-16T07:01:58.254923Z","iopub.status.idle":"2025-02-16T07:01:58.258852Z","shell.execute_reply.started":"2025-02-16T07:01:58.2549Z","shell.execute_reply":"2025-02-16T07:01:58.257923Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Open video\ncap = cv2.VideoCapture(input_video_path)\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    processed_frame = process_frame_track_deepsort(frame)\n    out.write(processed_frame)\n\ncap.release()\nout.release()\n\nprint(f\"Processed video saved to: {output_video_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T05:57:47.881359Z","iopub.execute_input":"2025-02-16T05:57:47.881722Z","iopub.status.idle":"2025-02-16T05:59:22.255172Z","shell.execute_reply.started":"2025-02-16T05:57:47.881697Z","shell.execute_reply":"2025-02-16T05:59:22.254358Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3/13/25","metadata":{}},{"cell_type":"code","source":"def process_frame(frame):\n    height, width = frame.shape[:2]\n    frame_area = width * height  # Total frame area\n\n    # === Step 1: Get YOLOv8 Predictions ===\n    results_v8 = model_v8.predict(frame, verbose=False, agnostic_nms=False, conf=0.25)[0]\n    boxes_v8 = results_v8.boxes.xyxy.cpu().numpy() / [width, height, width, height]  # Normalize\n    scores_v8 = results_v8.boxes.conf.cpu().numpy()\n    labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n    # === Step 2: Get YOLOv9 Predictions ===\n    results_v9 = model_v9.predict(frame, verbose=False, agnostic_nms=False, conf=0.25)[0]\n    boxes_v9 = results_v9.boxes.xyxy.cpu().numpy() / [width, height, width, height]  # Normalize\n    scores_v9 = results_v9.boxes.conf.cpu().numpy()\n    labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n\n    # === Step 3: Apply Weighted Box Fusion (WBF) ===\n    all_boxes = [boxes_v8, boxes_v9]\n    all_scores = [scores_v8, scores_v9]\n    all_labels = [labels_v8, labels_v9]\n    weights = [1.2, 1.8]  # Equal weight for both models\n\n    boxes_wbf, scores_wbf, labels_wbf = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels, weights=weights, iou_thr=0.5, skip_box_thr=0.1\n    )\n\n    # Convert back to original scale\n    boxes_wbf *= [width, height, width, height]\n    boxes_wbf = boxes_wbf.astype(int)\n    labels_wbf = labels_wbf.astype(int)  # Ensure integer labels\n\n    # === Step 4: Apply Soft-NMS ===\n    boxes_final, scores_final, labels_final = soft_nms(boxes_wbf, scores_wbf, labels_wbf)\n\n    # === Step 5: Filter by Confidence Threshold (>= 0.16) and Remove Large Screen-Filling Boxes ===\n    valid_indices = []\n    for i, (x1, y1, x2, y2) in enumerate(boxes_final):\n        box_area = (x2 - x1) * (y2 - y1)\n        if scores_final[i] >= 0.16 and box_area < 0.85 * frame_area:  # Remove huge boxes (> 85% screen area)\n            valid_indices.append(i)\n\n    boxes_final = boxes_final[valid_indices]\n    scores_final = scores_final[valid_indices]\n    labels_final = labels_final[valid_indices]\n\n    # === Step 6: Draw Final Detections ===\n    for box, score, label in zip(boxes_final, scores_final, labels_final):\n        x1, y1, x2, y2 = map(int, box)  # Ensure all coordinates are integers\n        \n        # Clamp coordinates to frame boundaries\n        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width - 1, x2), min(height - 1, y2)\n        \n        if x2 <= x1 or y2 <= y1:\n            continue  # Skip invalid boxes\n\n        # Get class name\n        class_name = class_names[label] if 0 <= label < len(class_names) else f\"Class_{label}\"\n\n        # Get color\n        color = class_colors.get(class_name, (0, 255, 0))  # Default green\n        color = tuple(map(int, color))  # Ensure integer values\n\n        # Draw bounding box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        # Label text\n        label_text = f\"{class_name}: {score:.2f}\"\n        \n        # Text size\n        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n\n        # Draw text background\n        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n\n        # Draw text\n        cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2, lineType=cv2.LINE_AA)\n\n    return frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:15:09.648414Z","iopub.execute_input":"2025-03-16T12:15:09.648727Z","iopub.status.idle":"2025-03-16T12:15:09.658874Z","shell.execute_reply.started":"2025-03-16T12:15:09.648702Z","shell.execute_reply":"2025-03-16T12:15:09.658184Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_36.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing1/13march(36-modification-1).mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T11:48:41.458585Z","iopub.execute_input":"2025-03-16T11:48:41.45891Z","iopub.status.idle":"2025-03-16T11:48:41.462567Z","shell.execute_reply.started":"2025-03-16T11:48:41.458884Z","shell.execute_reply":"2025-03-16T11:48:41.46161Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"cap = cv2.VideoCapture(input_video_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Process frame with WBF\n    processed_frame = process_frame(frame)\n\n    # Write frame to output video\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f'Output video saved to {output_video_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T13:14:08.269737Z","iopub.execute_input":"2025-03-13T13:14:08.270064Z","iopub.status.idle":"2025-03-13T13:15:05.253688Z","shell.execute_reply.started":"2025-03-13T13:14:08.270039Z","shell.execute_reply":"2025-03-13T13:15:05.252757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # Test Time Augmentation # ","metadata":{}},{"cell_type":"code","source":"import albumentations as A","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:26:40.23938Z","iopub.execute_input":"2025-03-13T14:26:40.23974Z","iopub.status.idle":"2025-03-13T14:26:41.626041Z","shell.execute_reply.started":"2025-03-13T14:26:40.239716Z","shell.execute_reply":"2025-03-13T14:26:41.625176Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define TTA transformations\ntta_transforms = [\n    lambda img: img,  # Original\n    lambda img: cv2.flip(img, 1),  # Horizontal Flip\n    lambda img: A.ShiftScaleRotate(scale_limit=0.1, rotate_limit=10, p=1)(image=img)['image'],  # Small Rotate & Scale\n]\n\ndef reverse_tta_boxes(boxes, width, height, tta_index):\n    \"\"\"Reverse the augmentation applied to the bounding boxes.\"\"\"\n    if tta_index == 1:  # Horizontal Flip\n        boxes[:, [0, 2]] = width - boxes[:, [2, 0]]  # Flip x-coordinates\n    return boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:27:00.008412Z","iopub.execute_input":"2025-03-13T14:27:00.008716Z","iopub.status.idle":"2025-03-13T14:27:00.013577Z","shell.execute_reply.started":"2025-03-13T14:27:00.008694Z","shell.execute_reply":"2025-03-13T14:27:00.012631Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def process_frame(frame):\n    height, width = frame.shape[:2]\n    frame_area = width * height  \n\n    all_boxes_v8, all_scores_v8, all_labels_v8 = [], [], []\n    all_boxes_v9, all_scores_v9, all_labels_v9 = [], [], []\n\n    # === Step 1: Run TTA for YOLOv8 & YOLOv9 ===\n    for tta_index, transform in enumerate(tta_transforms):\n        aug_frame = transform(frame.copy())\n\n        # YOLOv8\n        results_v8 = model_v8.predict(aug_frame, verbose=False, agnostic_nms=False, conf=0.25)[0]\n        boxes_v8 = results_v8.boxes.xyxy.cpu().numpy() / [width, height, width, height]  \n        scores_v8 = results_v8.boxes.conf.cpu().numpy()\n        labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n        # YOLOv9\n        results_v9 = model_v9.predict(aug_frame, verbose=False, agnostic_nms=False, conf=0.25)[0]\n        boxes_v9 = results_v9.boxes.xyxy.cpu().numpy() / [width, height, width, height]  \n        scores_v9 = results_v9.boxes.conf.cpu().numpy()\n        labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n\n        # Reverse TTA for boxes\n        boxes_v8 = reverse_tta_boxes(boxes_v8, width, height, tta_index)\n        boxes_v9 = reverse_tta_boxes(boxes_v9, width, height, tta_index)\n\n        # Store results\n        all_boxes_v8.append(boxes_v8), all_scores_v8.append(scores_v8), all_labels_v8.append(labels_v8)\n        all_boxes_v9.append(boxes_v9), all_scores_v9.append(scores_v9), all_labels_v9.append(labels_v9)\n\n    # === Step 2: Apply Weighted Box Fusion (WBF) ===\n    weights = [1.2, 1.2, 1.2, 1.8, 1.8, 1.8]  # 2 models × 3 augmentations\n\n    boxes_wbf, scores_wbf, labels_wbf = weighted_boxes_fusion(\n    all_boxes_v8 + all_boxes_v9, \n    all_scores_v8 + all_scores_v9, \n    all_labels_v8 + all_labels_v9, \n    weights=weights, iou_thr=0.5, skip_box_thr=0.1\n)\n\n    # Convert to original scale\n    boxes_wbf *= [width, height, width, height]\n    boxes_wbf = boxes_wbf.astype(int)\n    labels_wbf = labels_wbf.astype(int)\n\n    # === Step 3: Apply Soft-NMS ===\n    boxes_final, scores_final, labels_final = soft_nms(boxes_wbf, scores_wbf, labels_wbf)\n\n    # === Step 4: Filter by Confidence & Remove Large Boxes ===\n    valid_indices = []\n    for i, (x1, y1, x2, y2) in enumerate(boxes_final):\n        box_area = (x2 - x1) * (y2 - y1)\n        if scores_final[i] >= 0.16 and box_area < 0.85 * frame_area:\n            valid_indices.append(i)\n\n    boxes_final = boxes_final[valid_indices]\n    scores_final = scores_final[valid_indices]\n    labels_final = labels_final[valid_indices]\n\n    # === Step 5: Draw Final Detections ===\n    for box, score, label in zip(boxes_final, scores_final, labels_final):\n        x1, y1, x2, y2 = map(int, box)  \n        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width - 1, x2), min(height - 1, y2)\n\n        if x2 <= x1 or y2 <= y1:\n            continue  \n\n        class_name = class_names[label] if 0 <= label < len(class_names) else f\"Class_{label}\"\n        color = class_colors.get(class_name, (0, 255, 0))  \n        color = tuple(map(int, color))  \n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        label_text = f\"{class_name}: {score:.2f}\"\n        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n        cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2, lineType=cv2.LINE_AA)\n\n    return frame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:30:27.80719Z","iopub.execute_input":"2025-03-13T14:30:27.807498Z","iopub.status.idle":"2025-03-13T14:30:27.819296Z","shell.execute_reply.started":"2025-03-13T14:30:27.807476Z","shell.execute_reply":"2025-03-13T14:30:27.818395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_36.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing1/13march(36-tta-2).mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:30:45.570411Z","iopub.execute_input":"2025-03-13T14:30:45.570733Z","iopub.status.idle":"2025-03-13T14:30:45.574294Z","shell.execute_reply.started":"2025-03-13T14:30:45.570706Z","shell.execute_reply":"2025-03-13T14:30:45.573472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cap = cv2.VideoCapture(input_video_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Process frame with WBF\n    processed_frame = process_frame(frame)\n\n    # Write frame to output video\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f'Output video saved to {output_video_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T14:30:51.480739Z","iopub.execute_input":"2025-03-13T14:30:51.481056Z","iopub.status.idle":"2025-03-13T14:33:09.931396Z","shell.execute_reply.started":"2025-03-13T14:30:51.481029Z","shell.execute_reply":"2025-03-13T14:33:09.930558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SAHI","metadata":{}},{"cell_type":"code","source":"import cv2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:15:14.776875Z","iopub.execute_input":"2025-03-16T12:15:14.777163Z","iopub.status.idle":"2025-03-16T12:15:14.780596Z","shell.execute_reply.started":"2025-03-16T12:15:14.777143Z","shell.execute_reply":"2025-03-16T12:15:14.779829Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"!pip install sahi ultralytics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:15:15.05258Z","iopub.execute_input":"2025-03-16T12:15:15.052801Z","iopub.status.idle":"2025-03-16T12:15:20.981645Z","shell.execute_reply.started":"2025-03-16T12:15:15.052784Z","shell.execute_reply":"2025-03-16T12:15:20.980759Z"}},"outputs":[{"name":"stdout","text":"Collecting sahi\n  Downloading sahi-0.11.22-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.3.91)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sahi) (8.1.7)\nCollecting fire (from sahi)\n  Downloading fire-0.7.0.tar.gz (87 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: opencv-python<=4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from sahi) (4.10.0.84)\nRequirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (11.0.0)\nCollecting pybboxes==0.1.6 (from sahi)\n  Downloading pybboxes-0.1.6-py3-none-any.whl.metadata (9.9 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from sahi) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from sahi) (2.32.3)\nRequirement already satisfied: shapely>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sahi) (2.0.6)\nCollecting terminaltables (from sahi)\n  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from sahi) (4.67.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pybboxes==0.1.6->sahi) (1.26.4)\nRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\nRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.5.1+cu121)\nRequirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.20.1+cu121)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\nRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.2.2)\nRequirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.14)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pybboxes==0.1.6->sahi) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pybboxes==0.1.6->sahi) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pybboxes==0.1.6->sahi) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pybboxes==0.1.6->sahi) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pybboxes==0.1.6->sahi) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pybboxes==0.1.6->sahi) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->sahi) (2024.12.14)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.16.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->sahi) (2.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pybboxes==0.1.6->sahi) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pybboxes==0.1.6->sahi) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pybboxes==0.1.6->sahi) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pybboxes==0.1.6->sahi) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pybboxes==0.1.6->sahi) (2024.2.0)\nDownloading sahi-0.11.22-py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pybboxes-0.1.6-py3-none-any.whl (24 kB)\nDownloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\nBuilding wheels for collected packages: fire\n  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=bde872a5d0e854c5906a177590f7c8ed87ea4158dd74a8fef69c24fc473f1a42\n  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\nSuccessfully built fire\nInstalling collected packages: terminaltables, fire, pybboxes, sahi\nSuccessfully installed fire-0.7.0 pybboxes-0.1.6 sahi-0.11.22 terminaltables-3.1.10\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from sahi import AutoDetectionModel\nfrom sahi.predict import get_sliced_prediction\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:15:20.982962Z","iopub.execute_input":"2025-03-16T12:15:20.98323Z","iopub.status.idle":"2025-03-16T12:15:21.160619Z","shell.execute_reply.started":"2025-03-16T12:15:20.983207Z","shell.execute_reply":"2025-03-16T12:15:21.159928Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def process_frame(frame):\n    height, width = frame.shape[:2]\n    frame_area = width * height\n    alpha = 0.6  # From YOSCA paper [Source 2]\n\n    # ===== 0. Load Original Models for Baseline =====\n    model_v8 = YOLO(\"/kaggle/input/yolov8besr/keras/default/1/best (1).pt\")  # Original YOLOv8\n    model_v9 = YOLO(\"/kaggle/input/yolov9bestt/keras/default/1/yolov9best.pt\")  # Original YOLOv9\n\n    # ===== 1. SAHI-Enhanced YOLOv8 Predictions =====\n    sahiv8_model = AutoDetectionModel.from_pretrained(\n        model_type=\"yolov8\",\n        model_path=\"/kaggle/input/yolov8besr/keras/default/1/best (1).pt\",\n        confidence_threshold=0.25,\n        device=\"cuda\",\n        image_size=640,\n    )\n    results_v8 = get_sliced_prediction(\n        frame, sahiv8_model,\n        slice_height=896, slice_width=896,\n        overlap_height_ratio=0.2, overlap_width_ratio=0.2,\n        verbose=0, postprocess_match_threshold=0.5\n    )\n\n    # Get baseline YOLOv8 predictions\n    baseline_v8 = model_v8.predict(frame, verbose=False, conf=0.25, imgsz=640)[0]\n    baseline_scores_v8 = baseline_v8.boxes.conf.cpu().numpy()\n    mean_baseline_v8 = baseline_scores_v8.mean() if len(baseline_scores_v8) > 0 else 0.0\n\n    # Handle SAHI predictions with YOSCA adjustment\n    if len(results_v8.object_prediction_list) > 0:\n        boxes_v8 = np.array([obj.bbox.to_xyxy() for obj in results_v8.object_prediction_list]) / [width, height, width, height]\n        boxes_v8 = boxes_v8.reshape(-1, 4)\n        scores_v8 = np.array([obj.score.value for obj in results_v8.object_prediction_list])\n        scores_v8 = alpha * scores_v8 + (1 - alpha) * mean_baseline_v8  # YOSCA blending\n        labels_v8 = np.array([obj.category.id for obj in results_v8.object_prediction_list])\n    else:\n        boxes_v8 = np.empty((0, 4))\n        scores_v8 = np.array([])\n        labels_v8 = np.array([])\n\n    # ===== 2. SAHI-Enhanced YOLOv9 Predictions ===== \n    sahiv9_model = AutoDetectionModel.from_pretrained(\n        model_type=\"ultralytics\",\n        model_path=\"/kaggle/input/yolov9bestt/keras/default/1/yolov9best.pt\",\n        confidence_threshold=0.25,\n        device=\"cuda\",\n        image_size=640\n    )\n    results_v9 = get_sliced_prediction(\n        frame, sahiv9_model,\n        slice_height=896, slice_width=896,\n        overlap_height_ratio=0.2, overlap_width_ratio=0.2,\n        verbose=0, postprocess_match_threshold=0.5\n    )\n\n    # Get baseline YOLOv9 predictions\n    baseline_v9 = model_v9.predict(frame, verbose=False, conf=0.25, imgsz=640)[0]\n    baseline_scores_v9 = baseline_v9.boxes.conf.cpu().numpy()\n    mean_baseline_v9 = baseline_scores_v9.mean() if len(baseline_scores_v9) > 0 else 0.0\n\n    if len(results_v9.object_prediction_list) > 0:\n        boxes_v9 = np.array([obj.bbox.to_xyxy() for obj in results_v9.object_prediction_list]) / [width, height, width, height]\n        boxes_v9 = boxes_v9.reshape(-1, 4)\n        scores_v9 = np.array([obj.score.value for obj in results_v9.object_prediction_list])\n        scores_v9 = alpha * scores_v9 + (1 - alpha) * mean_baseline_v9  # YOSCA blending\n        labels_v9 = np.array([obj.category.id for obj in results_v9.object_prediction_list])\n    else:\n        boxes_v9 = np.empty((0, 4))\n        scores_v9 = np.array([])\n        labels_v9 = np.array([])\n                             \n    # === Step 3: Apply Weighted Box Fusion (WBF) ===\n    all_boxes = [boxes_v8, boxes_v9]\n    all_scores = [scores_v8, scores_v9]\n    all_labels = [labels_v8, labels_v9]\n    weights = [1.2, 1.8]  \n\n    boxes_wbf, scores_wbf, labels_wbf = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels, weights=weights, iou_thr=0.5, skip_box_thr=0.1\n    )\n\n    # Convert back to original scale\n    boxes_wbf *= [width, height, width, height]\n    boxes_wbf = boxes_wbf.astype(int)\n    labels_wbf = labels_wbf.astype(int)  # Ensure integer labels\n\n    # === Step 4: Apply Soft-NMS ===\n    boxes_final, scores_final, labels_final = soft_nms(boxes_wbf, scores_wbf, labels_wbf)\n\n    # === Step 5: Filter by Confidence Threshold (>= 0.16) and Remove Large Screen-Filling Boxes ===\n    valid_indices = []\n    for i, (x1, y1, x2, y2) in enumerate(boxes_final):\n        box_area = (x2 - x1) * (y2 - y1)\n        if scores_final[i] >= 0.16 and box_area < 0.85 * frame_area:  # Remove huge boxes (> 85% screen area)\n            valid_indices.append(i)\n\n    boxes_final = boxes_final[valid_indices]\n    scores_final = scores_final[valid_indices]\n    labels_final = labels_final[valid_indices]\n\n    # === Step 6: Draw Final Detections ===\n    for box, score, label in zip(boxes_final, scores_final, labels_final):\n        x1, y1, x2, y2 = map(int, box)  # Ensure all coordinates are integers\n        \n        # Clamp coordinates to frame boundaries\n        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width - 1, x2), min(height - 1, y2)\n        \n        if x2 <= x1 or y2 <= y1:\n            continue  # Skip invalid boxes\n\n        # Get class name\n        class_name = class_names[label] if 0 <= label < len(class_names) else f\"Class_{label}\"\n\n        # Get color\n        color = class_colors.get(class_name, (0, 255, 0))  # Default green\n        color = tuple(map(int, color))  # Ensure integer values\n\n        # Draw bounding box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        # Label text\n        label_text = f\"{class_name}: {score:.2f}\"\n        \n        # Text size\n        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n\n        # Draw text background\n        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n\n        # Draw text\n        cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2, lineType=cv2.LINE_AA)\n\n    return frame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:14:35.612143Z","iopub.execute_input":"2025-03-16T06:14:35.613278Z","iopub.status.idle":"2025-03-16T06:14:35.632654Z","shell.execute_reply.started":"2025-03-16T06:14:35.613229Z","shell.execute_reply":"2025-03-16T06:14:35.631505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_36.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing1/15march(36-modification-2).mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:14:39.697231Z","iopub.execute_input":"2025-03-16T06:14:39.697565Z","iopub.status.idle":"2025-03-16T06:14:39.701735Z","shell.execute_reply.started":"2025-03-16T06:14:39.697539Z","shell.execute_reply":"2025-03-16T06:14:39.700696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cap = cv2.VideoCapture(input_video_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Process frame with WBF\n    processed_frame = process_frame(frame)\n\n    # Write frame to output video\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f'Output video saved to {output_video_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:14:41.202789Z","iopub.execute_input":"2025-03-16T06:14:41.20315Z","iopub.status.idle":"2025-03-16T06:57:12.305707Z","shell.execute_reply.started":"2025-03-16T06:14:41.203125Z","shell.execute_reply":"2025-03-16T06:57:12.304786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport subprocess\nfrom IPython.display import FileLink, display\n\ndef download_file(path, download_file_name):\n    os.chdir('/kaggle/working/')\n    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n    \n    # Enclose the path in double quotes to handle special characters\n    command = f'zip -r \"{zip_name}\" \"{path}\"'\n\n    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n    if result.returncode != 0:\n        print(\"Unable to run zip command!\")\n        print(result.stderr)\n        return\n    \n    display(FileLink(f\"{download_file_name}.zip\"))\n\n# Call the function correctly\ndownload_file('/kaggle/working/testing1/15march(36-modification-2).mp4', 'out')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T06:59:32.862545Z","iopub.execute_input":"2025-03-16T06:59:32.86292Z","iopub.status.idle":"2025-03-16T06:59:37.356586Z","shell.execute_reply.started":"2025-03-16T06:59:32.862895Z","shell.execute_reply":"2025-03-16T06:59:37.355789Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Shadows","metadata":{}},{"cell_type":"code","source":"!pip install CuPy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:42:32.85552Z","iopub.execute_input":"2025-03-16T12:42:32.855806Z","iopub.status.idle":"2025-03-16T12:42:46.711794Z","shell.execute_reply.started":"2025-03-16T12:42:32.855784Z","shell.execute_reply":"2025-03-16T12:42:46.710688Z"}},"outputs":[{"name":"stdout","text":"Collecting CuPy\n  Downloading cupy-13.4.0.tar.gz (3.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy<2.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from CuPy) (1.26.4)\nRequirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from CuPy) (0.8.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22->CuPy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22->CuPy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22->CuPy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22->CuPy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22->CuPy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.3,>=1.22->CuPy) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22->CuPy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.3,>=1.22->CuPy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.3,>=1.22->CuPy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.3,>=1.22->CuPy) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.3,>=1.22->CuPy) (2024.2.0)\nBuilding wheels for collected packages: CuPy\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Building wheel for CuPy (setup.py) ... \u001b[?25lerror\n\u001b[31m  ERROR: Failed building wheel for CuPy\u001b[0m\u001b[31m\n\u001b[0m\u001b[?25h  Running setup.py clean for CuPy\n  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m×\u001b[0m \u001b[32mpython setup.py clean\u001b[0m did not run successfully.\n  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m╰─>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n\u001b[31m  ERROR: Failed cleaning build dir for CuPy\u001b[0m\u001b[31m\n\u001b[0mFailed to build CuPy\n\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (CuPy)\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"def adaptive_gamma_correction(image, gamma_min=0.5, gamma_max=2.5):\n    \"\"\"Automatically adjusts gamma based on image brightness.\"\"\"\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    mean_intensity = np.mean(gray) / 255.0  # Normalize to [0, 1]\n\n    # Compute adaptive gamma (inverse relationship with brightness)\n    gamma = gamma_max - (gamma_max - gamma_min) * mean_intensity\n\n    # Apply gamma correction\n    inv_gamma = 1.0 / gamma\n    table = np.array([(i / 255.0) ** inv_gamma * 255 for i in range(256)]).astype(\"uint8\")\n    corrected = cv2.LUT(image, table)\n\n    return corrected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T12:43:15.345522Z","iopub.execute_input":"2025-03-16T12:43:15.345827Z","iopub.status.idle":"2025-03-16T12:43:15.35084Z","shell.execute_reply.started":"2025-03-16T12:43:15.345801Z","shell.execute_reply":"2025-03-16T12:43:15.349956Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def process_frame(frame):\n    height, width = frame.shape[:2]\n    frame_area = width * height  \n\n    # === Step 1: Apply Adaptive Gamma Correction (AGC) ===\n    frame = adaptive_gamma_correction(frame)  # Already implemented by you\n\n    # === Step 2: Get YOLOv8 Predictions ===\n    results_v8 = model_v8.predict(frame, verbose=False, agnostic_nms=False, conf=0.25)[0]\n    boxes_v8 = results_v8.boxes.xyxy.cpu().numpy()\n    scores_v8 = results_v8.boxes.conf.cpu().numpy()\n    labels_v8 = results_v8.boxes.cls.cpu().numpy().astype(int)\n\n    # === Step 3: Get YOLOv9 Predictions ===\n    results_v9 = model_v9.predict(frame, verbose=False, agnostic_nms=False, conf=0.25)[0]\n    boxes_v9 = results_v9.boxes.xyxy.cpu().numpy()\n    scores_v9 = results_v9.boxes.conf.cpu().numpy()\n    labels_v9 = results_v9.boxes.cls.cpu().numpy().astype(int)\n    \n    # === Step 4: Apply Weighted Box Fusion (WBF) ===\n    # Normalize YOLOv8 boxes\n    boxes_v8[:, [0, 2]] /= width   # Normalize X coordinates\n    boxes_v8[:, [1, 3]] /= height  # Normalize Y coordinates\n    \n    # Normalize YOLOv9 boxes\n    boxes_v9[:, [0, 2]] /= width\n    boxes_v9[:, [1, 3]] /= height\n    \n    all_boxes = [boxes_v8, boxes_v9]\n    all_scores = [scores_v8, scores_v9]\n    all_labels = [labels_v8, labels_v9]\n    weights = [1.2, 1.8]\n\n    boxes_wbf, scores_wbf, labels_wbf = weighted_boxes_fusion(\n        all_boxes, all_scores, all_labels, weights=weights, iou_thr=0.5, skip_box_thr=0.1\n    )\n\n    # Convert back to original scale\n    boxes_wbf = boxes_wbf.astype(int)\n    labels_wbf = labels_wbf.astype(int)  \n\n    # === Step 5: Apply Soft-NMS ===\n    boxes_final, scores_final, labels_final = soft_nms(boxes_wbf, scores_wbf, labels_wbf)\n\n    # === Step 6: Filter by Confidence Threshold (>= 0.16) and Remove Large Boxes ===\n    valid_indices = []\n    for i, (x1, y1, x2, y2) in enumerate(boxes_final):\n        box_area = (x2 - x1) * (y2 - y1)\n        if scores_final[i] >= 0.16 and box_area < 0.85 * frame_area:  \n            valid_indices.append(i)\n\n    boxes_final = boxes_final[valid_indices]\n    scores_final = scores_final[valid_indices]\n    labels_final = labels_final[valid_indices]\n\n    # === Step 7: Draw Final Detections ===\n    for box, score, label in zip(boxes_final, scores_final, labels_final):\n        x1, y1, x2, y2 = map(int, box)\n        x1, y1, x2, y2 = max(0, x1), max(0, y1), min(width - 1, x2), min(height - 1, y2)\n        \n        if x2 <= x1 or y2 <= y1:\n            continue  \n\n        class_name = class_names[label] if 0 <= label < len(class_names) else f\"Class_{label}\"\n        color = class_colors.get(class_name, (0, 255, 0))  \n        color = tuple(map(int, color))\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        label_text = f\"{class_name}: {score:.2f}\"\n        (text_width, text_height), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n\n        cv2.rectangle(frame, (x1, y1 - text_height - 10), (x1 + text_width, y1), color, -1)\n        cv2.putText(frame, label_text, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2, lineType=cv2.LINE_AA)\n\n    return frame\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:23:58.902466Z","iopub.execute_input":"2025-03-16T13:23:58.902806Z","iopub.status.idle":"2025-03-16T13:23:58.913981Z","shell.execute_reply.started":"2025-03-16T13:23:58.902782Z","shell.execute_reply":"2025-03-16T13:23:58.913094Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# Video processing\ninput_video_path = '/kaggle/input/videos/videos/scenevideo_2.mp4'  # Replace with your input video path\noutput_video_path = '/kaggle/working/testing1/15march(2(1)-modification-shadow).mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:24:03.460577Z","iopub.execute_input":"2025-03-16T13:24:03.460862Z","iopub.status.idle":"2025-03-16T13:24:03.464329Z","shell.execute_reply.started":"2025-03-16T13:24:03.460841Z","shell.execute_reply":"2025-03-16T13:24:03.463604Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"cap = cv2.VideoCapture(input_video_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Process frame with WBF\n    processed_frame = process_frame(frame)\n\n    # Write frame to output video\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f'Output video saved to {output_video_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T13:22:04.002205Z","iopub.execute_input":"2025-03-16T13:22:04.002546Z","iopub.status.idle":"2025-03-16T13:22:04.925797Z","shell.execute_reply.started":"2025-03-16T13:22:04.002519Z","shell.execute_reply":"2025-03-16T13:22:04.924609Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-48b2267f0348>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Process frame with WBF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mprocessed_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Write frame to output video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-48-88d54692b104>\u001b[0m in \u001b[0;36mprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mbrightness_v8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_boxes_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_v8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbrightness_v9\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_boxes_brightness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes_v9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mscores_v8\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrightness_v8\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbrightness_threshold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mconfidence_boost_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mscores_v9\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbrightness_v9\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbrightness_threshold\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mconfidence_boost_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mscores_v8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_v8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mcupy/_core/core.pyx\u001b[0m in \u001b[0;36mcupy._core.core._ndarray_base.__array__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly."],"ename":"TypeError","evalue":"Implicit conversion to a NumPy array is not allowed. Please use `.get()` to construct a NumPy array explicitly.","output_type":"error"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}